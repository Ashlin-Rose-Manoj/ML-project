[
  {
    "objectID": "Ml_notebook.html",
    "href": "Ml_notebook.html",
    "title": "End-to-End Python Environment",
    "section": "",
    "text": "In this project we used specific versions of the libraries. Save the following libraries in a text file- requirements.txt. Install all these libraries using the code python -r pip install requirements.txt.\njoblib==1.3.2\nstreamlit==1.31.1\nscikit-learn==1.2.2\npandas==2.0.0"
  },
  {
    "objectID": "Ml_notebook.html#set-up-ml-environment",
    "href": "Ml_notebook.html#set-up-ml-environment",
    "title": "End-to-End Python Environment",
    "section": "",
    "text": "In this project we used specific versions of the libraries. Save the following libraries in a text file- requirements.txt. Install all these libraries using the code python -r pip install requirements.txt.\njoblib==1.3.2\nstreamlit==1.31.1\nscikit-learn==1.2.2\npandas==2.0.0"
  },
  {
    "objectID": "Ml_notebook.html#end-to-end-machine-learning-project-classifying-the-iris-dataset",
    "href": "Ml_notebook.html#end-to-end-machine-learning-project-classifying-the-iris-dataset",
    "title": "End-to-End Python Environment",
    "section": "End-to-End Machine Learning Project: Classifying the Iris Dataset",
    "text": "End-to-End Machine Learning Project: Classifying the Iris Dataset\nIn this project, we will walk through an end-to-end machine learning task using the Iris dataset. This comprehensive exercise will cover all stages of a machine learning pipeline, from data exploration to model deployment."
  },
  {
    "objectID": "Ml_notebook.html#introduction-to-the-dataset",
    "href": "Ml_notebook.html#introduction-to-the-dataset",
    "title": "End-to-End Python Environment",
    "section": "1. Introduction to the Dataset",
    "text": "1. Introduction to the Dataset\nThe Iris dataset is a classic dataset in machine learning, widely used for benchmarking classification algorithms. It consists of measurements from 150 iris flowers, with four features- Sepal Length, Sepal Width, Petal Length, and Petal Width. Each sample is labeled with one of three species- Iris-setosa, Iris-versicolor, and Iris-virginica."
  },
  {
    "objectID": "Ml_notebook.html#objective",
    "href": "Ml_notebook.html#objective",
    "title": "End-to-End Python Environment",
    "section": "2. Objective",
    "text": "2. Objective\nOur objective is to build a classification model that can accurately predict the species of an iris flower based on its measurements. We will explore the dataset, perform necessary preprocessing, and select an appropriate classification algorithm to achieve this goal"
  },
  {
    "objectID": "Ml_notebook.html#data-exploration-and-preprocessing",
    "href": "Ml_notebook.html#data-exploration-and-preprocessing",
    "title": "End-to-End Python Environment",
    "section": "3. Data Exploration and Preprocessing",
    "text": "3. Data Exploration and Preprocessing\n\nExploratory Data Analysis (EDA): We will begin by analyzing the dataset to understand its structure and characteristics. This includes visualizing distributions, checking for missing values, and examining class balance. In this stage we need to load the dataset using appropriate python libraries. We want to follow a systematic approach to understand the dataset’s structure, clean the data, and gain insights. Here’s a step-by-step procedure for EDA using Python. As the first step let’s load necessary python libraries for this job.\n\n\n\nCode\n# loading necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nCode\n# used to supress warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nIn this EDA process, libraries such as pandas, seaborn, matplotlib, and scikit-learn are essential. Pandas is used for efficient data manipulation and preprocessing, allowing us to load, clean, and manage the dataset seamlessly. Seaborn and matplotlib provide advanced visualization capabilities to explore the distribution, outliers, and relationships among features, which are crucial for understanding the dataset’s structure and potential issues. Together, these libraries offer a comprehensive toolkit for conducting thorough exploratory data analysis, ensuring that the dataset is well-understood and ready for subsequent modeling.\nIn the next step, we load the Iris dataset directly from a remote URL using pandas. The code iris_df = pd.read_csv(‘https://raw.githubusercontent.com/sijuswamy/Model_Deployment/main/iris.csv’) reads the CSV file from the specified GitHub repository and creates a DataFrame named iris_df, which contains the dataset for further analysis.\n\n\nCode\n# loading dataset\niris_df=pd.read_csv('https://raw.githubusercontent.com/sijuswamy/Model_Deployment/main/iris.csv')\n\n\n\n\nCode\niris_df.head()\n\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nSetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nSetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nSetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nSetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nSetosa\n\n\n\n\n\n\n\n\n\nCode\niris_df.tail()\n\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\n145\n6.7\n3.0\n5.2\n2.3\nVirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nVirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nVirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nVirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nVirginica\n\n\n\n\n\n\n\nChecking the Dataset Shape: The code {python} iris_df.shape returns a tuple representing the dimensions of the iris_df DataFrame, indicating the number of rows and columns in the dataset."
  },
  {
    "objectID": "Ml_notebook.html#a-data-cleaning",
    "href": "Ml_notebook.html#a-data-cleaning",
    "title": "End-to-End Python Environment",
    "section": "3.a Data cleaning",
    "text": "3.a Data cleaning\nData Cleaning: We will handle any missing values and ensure the data is ready for modeling. Basic preprocessing tasks will include feature scaling and normalization. Various steps in this stage is explained below. Checking for Duplicates: The code iris_df.duplicated().sum() counts the number of duplicate rows in the iris_df DataFrame, helping identify any redundancy in the dataset that may need to be addressed.\n\n\n\n\n\n\nTip\n\n\n\nTip Checking for duplicates is important because duplicate rows can skew analysis, introduce bias, and affect the performance of machine learning models. By identifying and removing duplicates, we ensure that each observation is unique and that the dataset accurately represents the underlying data without redundancy.\n\n\nIdentifying Duplicate Rows: The code iris_df[iris_df.duplicated()] filters and displays the duplicate rows in the {python}iris_df DataFrame, allowing us to inspect and address any redundancy in the dataset by showing which rows are duplicated.\n\n\nCode\niris_df[iris_df.duplicated()]\n\n\n\n\n\n\n\n\n\nsepal.length\nsepal.width\npetal.length\npetal.width\nvariety\n\n\n\n\n142\n5.8\n2.7\n5.1\n1.9\nVirginica\n\n\n\n\n\n\n\nChecking Class Distribution and Data Imbalance: The code iris_df[‘variety’].value_counts() counts the number of occurrences of each unique value in the variety column of the {python}iris_df DataFrame, providing insight into the distribution of classes and helping to identify any class imbalances in the dataset.\n\n\nCode\niris_df['variety'].value_counts()\n\n\nvariety\nSetosa        50\nVersicolor    50\nVirginica     50\nName: count, dtype: int64\n\n\n\n\n\n\n\n\nCaution\n\n\n\nCaution An imbalanced dataset, where some classes are significantly underrepresented compared to others, can lead to biased model performance. It may cause the model to favor the majority class, resulting in poor predictive accuracy for the minority class and skewed evaluation metrics. Addressing class imbalance ensures that the model learns to generalize across all classes effectively, leading to more reliable and fair predictions.\n\n\nChecking for Missing Values: The code iris_df.isnull().sum(axis=0) calculates the number of missing values for each column in the {python}iris_df DataFrame, helping to identify and address any gaps in the dataset that may need to be handled before analysis or modeling.\n\n\nCode\niris_df.isnull().sum(axis=0)\n\n\nsepal.length    0\nsepal.width     0\npetal.length    0\npetal.width     0\nvariety         0\ndtype: int64\n\n\nChecking for missing values is essential because missing data can compromise the integrity of the analysis and modeling process. By identifying columns with missing values, we can take appropriate steps to handle them—such as imputation or removal—ensuring that the dataset is complete and reliable for generating accurate insights and predictions.\nStatistical summary: Checking skewness, kurtosis, and correlation is essential for understanding data distribution and feature relationships. Skewness measures asymmetry; values between -0.5 and 0.5 indicate a fairly normal distribution, while values beyond this range suggest significant skewness. Kurtosis assesses the heaviness of tails; values close to 3 indicate a normal distribution, while values much higher or lower suggest the presence or absence of outliers, respectively. Correlation examines feature relationships, with values close to 1 or -1 indicating strong correlations that could lead to multicollinearity. Analyzing these metrics helps in identifying data transformation needs, managing outliers, and optimizing feature selection, ultimately improving model performance and reliability. Before performing the statistical operations, check for the categorical variables. If so remove them and apply statistical operations on that pruned dataset. The following code will do that.\n\n\nCode\n# Check if 'variety' column exists in the DataFrame\nif 'variety' in iris_df.columns:\n    removed_col = iris_df[\"variety\"]\n    iris_num = iris_df.drop('variety', axis=1)  # Use drop to remove the column and keep the rest\n    print(\"Successfully removed 'variety' column.\")\nelse:\n    print(\"Column 'variety' not found in the DataFrame.\")\n\n\nSuccessfully removed 'variety' column."
  },
  {
    "objectID": "Ml_notebook.html#e-statistical-feature-analysis",
    "href": "Ml_notebook.html#e-statistical-feature-analysis",
    "title": "End-to-End Python Environment",
    "section": "3.e Statistical Feature Analysis",
    "text": "3.e Statistical Feature Analysis\nIn this section various features are analysed in more detail. Presence of outlair and normality of feature distribution will be checked before ML model building.\nBoxplots for Feature Analysis: The code {python}fig, axes = plt.subplots(2, 2, figsize=(16,9)) creates a 2x2 grid of subplots with a figure size of 16x9 inches. Each {python}sns.boxplot() function call plots the distribution of a specific feature {python}(petal.width, petal.length, sepal.length, sepal.width) against the variety of the iris species. The orient='v' parameter specifies vertical boxplots. This visualization helps in comparing the distributions of different features across species, highlighting differences in feature ranges, central tendencies, and potential outliers. The plt.show() command displays all the plots.\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(6,5))\nsns.boxplot(  y=\"petal.width\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[0, 0])\nsns.boxplot(  y=\"petal.length\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[0, 1])\nsns.boxplot(  y=\"sepal.length\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[1, 0])\nsns.boxplot(  y=\"sepal.width\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[1, 1])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(6,5))\nsns.violinplot(y=\"petal.width\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[0, 0])\nsns.violinplot(y=\"petal.length\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[0, 1])\nsns.violinplot(y=\"sepal.length\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[1, 0])\nsns.violinplot(y=\"sepal.length\", x= \"variety\", data=iris_df,  orient='v' , ax=axes[1, 1])\nplt.show()"
  },
  {
    "objectID": "Ml_notebook.html#model-identification-and-training",
    "href": "Ml_notebook.html#model-identification-and-training",
    "title": "End-to-End Python Environment",
    "section": "4. Model Identification and Training",
    "text": "4. Model Identification and Training\nBased on our data exploration, we will select a suitable classification model. Based on the Exploratory Data Analysis (EDA), we found that petal.length and petal.width are the most influential features for determining the variety of the Iris flower. To classify the Iris dataset, several classification models can be employed. In this discussion, we will consider Logistic Regression, K-nn, Support Vector Machine, Decision Tree and Random Forest algorithms.\n1. Logistic Regression\nLogistic Regression is a simple yet effective classification algorithm that models the probability of a class label based on input features. It’s suitable for binary and multiclass classification problems and works well when the relationship between features and target is approximately linear.\n2. k-Nearest Neighbors (k-NN)\nk-Nearest Neighbors is a non-parametric method used for classification. It works by finding the k nearest data points to a given point and assigning the class that is most common among these neighbors. It is effective for datasets where the decision boundary is non-linear.\n3. Support Vector Machine (SVM)\nSupport Vector Machine is a powerful classification technique that works by finding the hyperplane that best separates the classes in the feature space. It is well-suited for datasets with a clear margin of separation and can handle both linear and non-linear classification tasks using kernel tricks.\n4. Decision Tree\nDecision Tree is a model that splits the data into subsets based on the value of input features, creating a tree-like structure of decisions. It is useful for handling both categorical and numerical data and provides a clear model interpretability.\n5. Random Forest\nRandom Forest is an ensemble method that combines multiple decision trees to improve classification performance. It reduces overfitting and improves accuracy by averaging the predictions from multiple trees, making it robust and effective for complex datasets.\nImporting Required Libraries\nTo perform machine learning tasks and evaluate model performance, the following libraries are imported\n\n\nCode\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport joblib\n\n\n\n\nCode\n# random seed\nseed = 42\niris_df.sample(frac=1, random_state=seed)\n\n# selecting features and target data\nX = iris_df[['sepal.length',    'sepal.width',  'petal.length', 'petal.width']]\ny = iris_df[['variety']]\n\n# split data into train and test sets\n# 70% training and 30% test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=seed, stratify=y)\n\n\n1. Logistic Regression To Train and Evaluate the Logistic Regression Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the LogisticRegression instance from the sklearnlibrary. This can be done as fol\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\n\n\nStep-2: Initialize the Model\nCreate an instance of the Logistic Regression model:\n\n\nCode\nmodel = LogisticRegression(max_iter=200)\n\n\nStep-3” 3. Train the Model\nFit the model to the training data:\n\n\nCode\nmodel.fit(X_train, y_train.values.ravel())\n\n\nLogisticRegression(max_iter=200)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(max_iter=200) \n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\n\n\nCode\ny_pred = model.predict(X_test)\n\n\nStep-5 Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n\nCode\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n# Generate classification report\nreport = classification_report(y_test, y_pred)\n\n\n\n\nCode\n# Plot confusion matrix as a heatmap\nplt.figure(figsize=(6,5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=y.columns, yticklabels=y.columns)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\n\n\nAccuracy: 0.93\nClassification Report:\n              precision    recall  f1-score   support\n\n      Setosa       1.00      1.00      1.00        15\n  Versicolor       0.88      0.93      0.90        15\n   Virginica       0.93      0.87      0.90        15\n\n    accuracy                           0.93        45\n   macro avg       0.93      0.93      0.93        45\nweighted avg       0.93      0.93      0.93        45\n\n\n\n5. RandonForest Classifier To Train and Evaluate the RandomForest Model, follow these steps.\nStep -1: Import Required Libraries Here we need only the Random Forest instance from the sklearn library. This can be done as follows.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nStep-2: Initialize the Model\nCreate an instance of the RandomForest model:\n\n\nCode\nRf_model = RandomForestClassifier(n_estimators=100, random_state=seed)\n\n\nStep-3: Train the Model\nFit the model to the training data:\n\n\nCode\nRf_model.fit(X_train, y_train.values.ravel())\n\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(random_state=42) \n\n\nStep-4: Make Predictions\nUse the trained model to predict the labels for the test set:\n\n\nCode\ny_pred = Rf_model.predict(X_test)\n\n\nStep-5: Evaluate the Model\nAssess the model’s performance using confussion matrix, model accuracy and a detailed classification report:\n\n\nCode\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\n# Generate confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n# Generate classification report\nreport = classification_report(y_test, y_pred)\n\n\n\n\nCode\n# Plot confusion matrix as a heatmap\nplt.figure(figsize=(6,5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=y.columns, yticklabels=y.columns)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\")\nprint(report)\n\n\nAccuracy: 0.89\nClassification Report:\n              precision    recall  f1-score   support\n\n      Setosa       1.00      1.00      1.00        15\n  Versicolor       0.78      0.93      0.85        15\n   Virginica       0.92      0.73      0.81        15\n\n    accuracy                           0.89        45\n   macro avg       0.90      0.89      0.89        45\nweighted avg       0.90      0.89      0.89        45\n\n\n\n5. Model Selection After training the model, we will evaluate its performance using various metrics such as accuracy and classification report. This will help us understand how well the model is performing and whether any improvements are needed. In this context, the RandomForestClassifier model is the winner. So we will select and save this model for deployment.\n\n\nCode\njoblib.dump(Rf_model, 'rf_model.sav')\n\n\n['rf_model.sav']"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML-project",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]